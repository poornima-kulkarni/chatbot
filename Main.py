from dotenv import load_dotenv
import streamlit as st
import os
import io
import requests
import google.generativeai as genai
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.lib.pagesizes import A4
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.enums import TA_LEFT
from reportlab.platypus import HRFlowable
from PIL import Image
from PIL import UnidentifiedImageError

# Load environment variables
load_dotenv()

# API keys
GEMINI_API_KEY = st.secrets["GOOGLE_API_KEY"]
HF_API_KEY = st.secrets["HF_API_KEY"]

# Configure Gemini
genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel("gemini-1.5-flash")
chat = model.start_chat(history=[])

# Load CSS
def load_css(file_name):
    try:
        with open(file_name) as f:
            st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)
    except FileNotFoundError:
        st.warning(f"CSS file '{file_name}' not found.")

# Display chat
def display_chat_message(role, message, is_user=False):
    css_class = "user-message" if is_user else "bot-message"
    st.markdown(f"""
    <div class="chat-message {css_class} fade-in">
        <b>{role}:</b> {message}
    </div>
    """, unsafe_allow_html=True)

# Gemini response
def generate_image_with_huggingface(prompt):
    API_URL = "https://api-inference.huggingface.co/models/stabilityai/stable-diffusion-2-1"
    headers = {
        "Authorization": f"Bearer {st.secrets['HF_API_KEY']}"
    }
    response = requests.post(API_URL, headers=headers, json={"inputs": prompt})
    if response.status_code == 200:
        image_bytes = response.content
        image = Image.open(io.BytesIO(image_bytes))
        return image
    else:
        return f"Error from Hugging Face API: {response.status_code} - {response.text}"

def get_gemini_response(prompt=None, files=None):
    contents = []
    if prompt:
        contents.append(prompt)

    # Check if user wants to generate an image
    if prompt and any(word in prompt.lower() for word in ["generate an image", "create an image", "draw", "image of", "visualize"]):
        try:
            image = generate_image_with_huggingface(prompt)
            if isinstance(image, Image.Image):
                st.image(image, caption="üé® Generated by Hugging Face (Stable Diffusion)", use_column_width=True)
                return "Here is the image generated from your prompt."
            else:
                return image  # This would be the error message
        except Exception as e:
            return f"Error generating image: {e}"

    # Handle other content (PDFs, DOCX, images)
    if files:
        for uploaded_file in files:
            file_type = uploaded_file.type
            file_bytes = uploaded_file.read()
            if "image" in file_type:
                try:
                    image = Image.open(io.BytesIO(file_bytes))
                    contents.append(image)
                except Exception as e:
                    st.error(f"Error processing image: {e}")
            elif file_type == "application/pdf":
                contents.append({"mime_type": "application/pdf", "data": file_bytes})
            elif "docx" in file_type:
                try:
                    from docx import Document
                    doc = Document(io.BytesIO(file_bytes))
                    full_text = [para.text for para in doc.paragraphs]
                    contents.append("\n".join(full_text))
                except Exception as e:
                    st.error(f"Error processing docx file: {e}")
                    contents.append(file_bytes.decode('latin-1', errors='ignore'))
            else:
                st.warning(f"Unsupported file type: {file_type}")

    if not contents:
        return ""

    response = chat.send_message(contents, stream=True)
    return "".join([chunk.text for chunk in response])

# Fetch Fal AI models
def get_falai_text_to_image_models():
    url = "https://huggingface.co/api/models"
    params = {"inference_provider": "fal-ai", "pipeline_tag": "text-to-image"}
    try:
        response = requests.get(url, params=params)
        response.raise_for_status()
        return [model["id"] for model in response.json()]
    except Exception as e:
        st.error(f"Failed to fetch Fal AI models: {e}")
        return []

# Generate image from text using selected model
def generate_image_from_prompt(model_id, prompt):
    url = f"https://api-inference.huggingface.co/models/{model_id}"
    headers = {
        "Authorization": f"Bearer {HF_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {"inputs": prompt}

    try:
        response = requests.post(url, headers=headers, json=payload)
        response.raise_for_status()
        if "image" in response.headers.get("content-type", ""):
            return Image.open(io.BytesIO(response.content))
        else:
            st.error(f"Unexpected response: {response.text}")
    except Exception as e:
        st.error(f"Image generation failed: {e}")
    return None

# Session state init
if 'chat_history' not in st.session_state:
    st.session_state['chat_history'] = []
if 'show_history' not in st.session_state:
    st.session_state['show_history'] = False
if 'last_response' not in st.session_state:
    st.session_state['last_response'] = ""
if 'falai_models' not in st.session_state:
    st.session_state['falai_models'] = []
if 'generated_image' not in st.session_state:
    st.session_state['generated_image'] = None

def toggle_history():
    st.session_state['show_history'] = not st.session_state['show_history']

def clear_response():
    st.session_state['last_response'] = ""

def clear_chat_history():
    st.session_state['chat_history'] = []
    st.session_state['show_history'] = False
    st.rerun()

# UI Setup
st.set_page_config(page_title="Chatbot", page_icon=":robot:", layout="wide", initial_sidebar_state="collapsed")
load_css('style.css')

st.markdown("""<h1 style='text-align: center;'>ü§ñ Ephemeral AI</h1>""", unsafe_allow_html=True)

with st.container():
    col1, col2 = st.columns([6, 2])

    # Sidebar Controls
    with col2:
        st.markdown("### üìä Chat Controls")

        if st.button("üìú Chat History"):
            toggle_history()

        download_format = st.selectbox("üìÅ Download Chat History", ["Select format", "Download as PDF", "Download as TXT"])
        chat_text = "\n".join([f"{role}: {text}" for role, text in st.session_state['chat_history']])

        if download_format == "Download as PDF":
            buffer = io.BytesIO()
            doc = SimpleDocTemplate(buffer, pagesize=A4, rightMargin=40, leftMargin=40, topMargin=40, bottomMargin=40)
            styles = getSampleStyleSheet()
            html_style = ParagraphStyle(name="HTMLStyle", parent=styles["Normal"], fontName="Helvetica", fontSize=11, leading=14, alignment=TA_LEFT)

            def markdown_to_html_bold(text):
                import re
                return re.sub(r'\*\*(.+?)\*\*', r'<b>\1</b>', text)

            story = []
            for line in markdown_to_html_bold(chat_text).split('\n'):
                if line.strip():
                    if "You:" in line:
                        line = f'<font color="green">{line}</font>'
                    elif "ü§ñ:" in line:
                        line = f'<font color="black">{line}</font>'
                        story.append(HRFlowable(width="100%", thickness=0.7, color="grey"))
                    story.append(Paragraph(line, html_style))
                else:
                    story.append(Spacer(1, 12))
            doc.build(story)
            buffer.seek(0)
            st.download_button("üìÑ Download PDF", buffer, file_name="chat_history.pdf", mime="application/pdf")

        elif download_format == "Download as TXT":
            buffer = io.BytesIO(chat_text.encode("utf-8"))
            st.download_button("üìÑ Download TXT", buffer, file_name="chat_history.txt", mime="text/plain")

        # Upload files
        st.markdown("<div class='file'><b>File Upload:</b></div>", unsafe_allow_html=True)
        uploaded_files = st.file_uploader("Upload files (images, PDFs, DOCX):", type=["png", "jpg", "jpeg", "pdf", "docx"], accept_multiple_files=True)

        

    # Chatbot input + display
    with col1:
        st.markdown("<div class='h4'><b>Type your prompt here:</b></div>", unsafe_allow_html=True)
        user_input = st.chat_input("Type your message here...")

        if user_input:
            with st.spinner("ü§î Thinking..."):
                response = get_gemini_response(prompt=user_input, files=uploaded_files)
                st.session_state['last_response'] = response

            user_msg = user_input
            if uploaded_files:
                file_names = [file.name for file in uploaded_files]
                user_msg += f" (Uploaded: {', '.join(file_names)})"

            st.session_state['chat_history'].append(("You", user_msg))
            st.session_state['chat_history'].append(("ü§ñ", st.session_state['last_response']))

            st.markdown(f"<div class='chat-message'><b>You:</b> {user_msg}</div>", unsafe_allow_html=True)
            st.markdown(f"<div class='bot'><b>ü§ñ:</b> {st.session_state['last_response']}</div>", unsafe_allow_html=True)

        if st.session_state['show_history'] and st.session_state['chat_history']:
            st.markdown("---")
            st.markdown("### üìú Chat History")
            for i in range(0, len(st.session_state['chat_history']), 2):
                user_msg = st.session_state['chat_history'][i]
                bot_msg = st.session_state['chat_history'][i + 1] if i + 1 < len(st.session_state['chat_history']) else ("ü§ñ", "")
                display_chat_message(user_msg[0], user_msg[1], is_user=True)
                display_chat_message("ü§ñ", bot_msg[1], is_user=False)
                st.markdown("---")
        # Fal AI Model Integration
        st.markdown("### üé® Generate Image with Fal AI")

        if st.button("üîç Fetch Fal AI Models"):
            with st.spinner("Fetching models..."):
                st.session_state['falai_models'] = get_falai_text_to_image_models()
        
        if st.session_state['falai_models']:
            selected_model = st.selectbox("üß† Select Model", st.session_state['falai_models'])
            text_prompt = st.text_input("üìù Enter image prompt:")
            if st.button("üé® Generate Image"):
                with st.spinner("Generating image..."):
                    image = generate_image_from_prompt(selected_model, text_prompt)
                    if image:
                        st.session_state['generated_image'] = image

        if st.session_state['generated_image']:
            st.image(st.session_state['generated_image'], caption="Generated Image", use_column_width=True)
            img_buf = io.BytesIO()
            st.session_state['generated_image'].save(img_buf, format="PNG")
            st.download_button("üì• Download Image", img_buf.getvalue(), "generated_image.png", mime="image/png")
# Sidebar extras
with st.sidebar:
    st.markdown("### üõ†Ô∏è Chat Controls")
    if st.button("üóëÔ∏è Clear Last Response"):
        clear_response()
        st.success("Last response cleared!")

    if st.button("üóëÔ∏è Clear Chat History"):
        clear_chat_history()

    st.markdown("---")
    st.markdown("### ‚ÑπÔ∏è Tips")
    st.markdown("""
    - Supports PDF, DOCX, and image uploads  
    - Use Fal AI to turn text into images  
    - Download chats or generated images  
    - All chat is temporary (ephemeral)
    """)

st.markdown("""<div class="bottom-note"
    <strong>üîí Privacy Notice:</strong> This is a temporary chat session‚Äîyour conversation history won't be saved to protect your privacy. 
    Want to keep your chat? Hit 'Download Chat History' before you refresh and lose it all.
</div>
""", unsafe_allow_html=True)

