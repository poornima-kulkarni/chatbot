from dotenv import load_dotenv
import streamlit as st
import os
import io
import requests
import google.generativeai as genai
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, HRFlowable
from reportlab.lib.pagesizes import A4
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.enums import TA_LEFT
from PIL import Image

# Load environment variables
load_dotenv()

# API keys
GEMINI_API_KEY = st.secrets["GOOGLE_API_KEY"]
HF_API_KEY = st.secrets["HF_API_KEY"]

# Configure Gemini
genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel("gemini-1.5-flash")
chat = model.start_chat(history=[])

# Load CSS
def load_css(file_name):
    try:
        with open(file_name) as f:
            st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)
    except FileNotFoundError:
        st.warning(f"CSS file '{file_name}' not found.")

# Display chat
def display_chat_message(role, message, is_user=False):
    css_class = "user-message" if is_user else "bot-message"
    st.markdown(f"""
    <div class="chat-message {css_class} fade-in">
        <b>{role}:</b> {message}
    </div>
    """, unsafe_allow_html=True)

# Gemini response handler
def get_gemini_response(prompt=None, files=None):
    contents = []
    if prompt:
        contents.append(prompt)

    # Auto image generation trigger
    if prompt and any(word in prompt.lower() for word in ["generate an image", "create an image", "draw", "image of", "visualize"]):
        try:
            image = generate_image_with_huggingface(prompt)
            if isinstance(image, Image.Image):
                st.image(image, caption="üé® Generated by Hugging Face (Stable Diffusion)", use_column_width=True)
                return "Here is the image generated from your prompt."
            else:
                return image
        except Exception as e:
            return f"Error generating image: {e}"

    # File handling
    if files:
        for uploaded_file in files:
            file_type = uploaded_file.type
            file_bytes = uploaded_file.read()
            if "image" in file_type:
                try:
                    image = Image.open(io.BytesIO(file_bytes))
                    contents.append(image)
                except Exception as e:
                    st.error(f"Error processing image: {e}")
            elif file_type == "application/pdf":
                contents.append({"mime_type": "application/pdf", "data": file_bytes})
            elif "docx" in file_type:
                try:
                    from docx import Document
                    doc = Document(io.BytesIO(file_bytes))
                    full_text = [para.text for para in doc.paragraphs]
                    contents.append("\n".join(full_text))
                except Exception as e:
                    st.error(f"Error processing docx: {e}")
                    contents.append(file_bytes.decode('latin-1', errors='ignore'))
            else:
                st.warning(f"Unsupported file type: {file_type}")

    if not contents:
        return ""

    response = chat.send_message(contents, stream=True)
    return "".join([chunk.text for chunk in response])

# Image generation via Hugging Face
def generate_image_with_huggingface(prompt):
    model_id = "prompthero/openjourney"  # Reliable fallback
    url = f"https://api-inference.huggingface.co/models/{model_id}"
    headers = {
        "Authorization": f"Bearer {HF_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {"inputs": prompt}
    try:
        response = requests.post(url, headers=headers, json=payload)
        response.raise_for_status()
        if "image" in response.headers.get("content-type", ""):
            return Image.open(io.BytesIO(response.content))
        else:
            return f"Unexpected response: {response.text}"
    except Exception as e:
        return f"Image generation failed: {e}"

# Session state init
for key in ["chat_history", "show_history", "last_response", "generated_image"]:
    if key not in st.session_state:
        st.session_state[key] = [] if "history" in key else False if "show" in key else ""

# Utilities
def toggle_history():
    st.session_state['show_history'] = not st.session_state['show_history']

def clear_response():
    st.session_state['last_response'] = ""

def clear_chat_history():
    st.session_state['chat_history'] = []
    st.session_state['show_history'] = False
    st.rerun()

# UI Setup
st.set_page_config(page_title="Chatbot", page_icon=":robot:", layout="wide", initial_sidebar_state="collapsed")
load_css("style.css")

st.markdown("<h1 style='text-align: center;'>ü§ñ Ephemeral AI</h1>", unsafe_allow_html=True)

with st.container():
    col1, col2 = st.columns([6, 2])

    # Sidebar Controls
    with col2:
        st.markdown("### üìä Chat Controls")
        if st.button("üìú Chat History"):
            toggle_history()

        download_format = st.selectbox("üìÅ Download Chat History", ["Select format", "Download as PDF", "Download as TXT"])
        chat_text = "\n".join([f"{role}: {text}" for role, text in st.session_state['chat_history']])

        if download_format == "Download as PDF":
            buffer = io.BytesIO()
            doc = SimpleDocTemplate(buffer, pagesize=A4, rightMargin=40, leftMargin=40, topMargin=40, bottomMargin=40)
            styles = getSampleStyleSheet()
            html_style = ParagraphStyle(name="HTMLStyle", parent=styles["Normal"], fontName="Helvetica", fontSize=11, leading=14, alignment=TA_LEFT)

            def markdown_to_html_bold(text):
                import re
                return re.sub(r'\*\*(.+?)\*\*', r'<b>\1</b>', text)

            story = []
            for line in markdown_to_html_bold(chat_text).split('\n'):
                if line.strip():
                    if "You:" in line:
                        line = f'<font color="green">{line}</font>'
                    elif "ü§ñ:" in line:
                        line = f'<font color="black">{line}</font>'
                        story.append(HRFlowable(width="100%", thickness=0.7, color="grey"))
                    story.append(Paragraph(line, html_style))
                else:
                    story.append(Spacer(1, 12))
            doc.build(story)
            buffer.seek(0)
            st.download_button("üìÑ Download PDF", buffer, file_name="chat_history.pdf", mime="application/pdf")

        elif download_format == "Download as TXT":
            buffer = io.BytesIO(chat_text.encode("utf-8"))
            st.download_button("üìÑ Download TXT", buffer, file_name="chat_history.txt", mime="text/plain")

        # File upload
        st.markdown("<div class='file'><b>File Upload:</b></div>", unsafe_allow_html=True)
        uploaded_files = st.file_uploader("Upload files (images, PDFs, DOCX):", type=["png", "jpg", "jpeg", "pdf", "docx"], accept_multiple_files=True)

    # Main Chat Column
    with col1:
        st.markdown("<div class='h4'><b>Type your prompt here:</b></div>", unsafe_allow_html=True)
        user_input = st.chat_input("Type your message here...", key="main_user_input")

        if user_input:
            with st.spinner("ü§î Thinking..."):
                response = get_gemini_response(prompt=user_input, files=uploaded_files)
                st.session_state['last_response'] = response

            user_msg = user_input
            if uploaded_files:
                file_names = [file.name for file in uploaded_files]
                user_msg += f" (Uploaded: {', '.join(file_names)})"

            st.session_state['chat_history'].append(("You", user_msg))
            st.session_state['chat_history'].append(("ü§ñ", st.session_state['last_response']))

            display_chat_message("You", user_msg, is_user=True)
            display_chat_message("ü§ñ", st.session_state['last_response'])

        if st.session_state['show_history'] and st.session_state['chat_history']:
            st.markdown("---")
            st.markdown("### üìú Chat History")
            for i in range(0, len(st.session_state['chat_history']), 2):
                user_msg = st.session_state['chat_history'][i]
                bot_msg = st.session_state['chat_history'][i + 1] if i + 1 < len(st.session_state['chat_history']) else ("ü§ñ", "")
                display_chat_message(user_msg[0], user_msg[1], is_user=True)
                display_chat_message("ü§ñ", bot_msg[1], is_user=False)
                st.markdown("---")

        # Fal AI Integration
        st.markdown("### üé® Generate Image with Text Prompt")
        image_prompt = st.text_input("Enter an image description:", key="image_prompt_input")

        if st.button("üé® Generate Image", key="generate_image_button") and image_prompt:
            with st.spinner("Generating image..."):
                image = generate_image_with_huggingface(image_prompt)
                if isinstance(image, Image.Image):
                    st.session_state['generated_image'] = image
                else:
                    st.error(image)

        if st.session_state['generated_image']:
            st.image(st.session_state['generated_image'], caption="Generated Image", use_column_width=True)
            img_buf = io.BytesIO()
            st.session_state['generated_image'].save(img_buf, format="PNG")
            st.download_button("üì• Download Image", img_buf.getvalue(), "generated_image.png", mime="image/png")

# Sidebar
with st.sidebar:
    st.markdown("### üõ†Ô∏è Chat Controls")
    if st.button("üóëÔ∏è Clear Last Response"):
        clear_response()
        st.success("Last response cleared!")

    if st.button("üóëÔ∏è Clear Chat History"):
        clear_chat_history()

    st.markdown("---")
    st.markdown("### ‚ÑπÔ∏è Tips")
    st.markdown("""
    - Upload PDFs, DOCX, or images  
    - Generate AI images from prompts  
    - Download your chat or image  
    - All data is ephemeral (temporary)
    """)

st.markdown("""<div class="bottom-note"
    <strong>üîí Privacy Notice:</strong> This is a temporary chat session‚Äîyour conversation history won't be saved. 
    Download it before refreshing.
</div>
""", unsafe_allow_html=True)
